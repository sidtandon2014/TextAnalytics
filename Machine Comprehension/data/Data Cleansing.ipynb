{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import tensorflow as tf\n",
    "\n",
    "#-----------Global variables\n",
    "GloveEmbeddings = {}\n",
    "max_query_words = 12\n",
    "max_passage_words = 50\n",
    "emb_dim = 100\n",
    "totalRowsInFile = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data.tsv\",sep=\"\\t\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadEmbeddings(embeddingfile):\n",
    "    global GloveEmbeddings,emb_dim\n",
    "\n",
    "    fe = open(embeddingfile,\"r\",encoding=\"utf-8\",errors=\"ignore\")\n",
    "    for line in fe:\n",
    "        tokens= line.strip().split()\n",
    "        word = tokens[0]\n",
    "        vec = list(map(float,tokens[1:]))\n",
    "        #vec = \" \".join(vec)\n",
    "        GloveEmbeddings[word]=vec\n",
    "    #Add Zerovec, this will be useful to pad zeros, it is better to experiment with padding any non-zero constant values also.\n",
    "    #GloveEmbeddings[\"zerovec\"] = \"0.0 \"*emb_dim\n",
    "    GloveEmbeddings[\"zerovec\"] = [0.0] *emb_dim\n",
    "    fe.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[float(word) for word in GloveEmbeddings[\"word\"].split()]\n",
    "#GloveEmbeddings[\"word\"] + GloveEmbeddings[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------Create multiple files for 1000 rows\n",
    "def removeStopWordsAndGenerateEmbeddings(inputfile,outputfile,isEvaluation):\n",
    "    def _bytes_feature(value):\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.compat.as_bytes(value)]))\n",
    "\n",
    "    def _int64_feature(value):\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "    \n",
    "    def _floatList_feature(value):\n",
    "        return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "    \n",
    "    import tensorflow as tf\n",
    "    global GloveEmbeddings,emb_dim,max_query_words,max_passage_words,totalRowsInFile\n",
    "    \n",
    "    f = open(inputfile,\"r\",encoding=\"utf-8\",errors=\"ignore\")  # Format of the file : query_id \\t query \\t passage \\t label \\t passage_id\n",
    "    \n",
    "    opts = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.ZLIB)\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    \n",
    "    fileCounter = 1 \n",
    "    currentRowNum = 0\n",
    "    checkpointRowNumber = 0\n",
    "    \n",
    "    #---------Create writer for first file\n",
    "    writer = tf.python_io.TFRecordWriter(outputfile.format(str(fileCounter)),opts)\n",
    "    \n",
    "    #---------Start reading input file\n",
    "    for line in f:\n",
    "        tokens = line.strip().lower().split(\"\\t\")\n",
    "        query_id,query,passage,label = [int(tokens[0])],tokens[1],tokens[2],int(tokens[3])\n",
    "\n",
    "        wordsWithoutPunctuation = re.split('\\W+', query)\n",
    "        words = [word for word in wordsWithoutPunctuation if word not in stop_words and len(word) > 0]\n",
    "        if len(words) == 0:\n",
    "            words = wordsWithoutPunctuation\n",
    "\n",
    "        word_count = len(words)\n",
    "        remaining = max_query_words - word_count  \n",
    "        if(remaining>0):\n",
    "            words += [\"zerovec\"]*remaining # Pad zero vecs if the word count is less than max_query_words\n",
    "        words = words[:max_query_words] # trim extra words\n",
    "        #create Query Feature vector \n",
    "        query_feature_vector = []\n",
    "        for word in words:\n",
    "            if(word in GloveEmbeddings):\n",
    "                query_feature_vector += GloveEmbeddings[word]\n",
    "            else:\n",
    "                query_feature_vector += GloveEmbeddings[\"zerovec\"]  #Add zerovec for OOV terms\n",
    "        #query_feature_vector = np.array(query_feature_vector.strip().split())\n",
    "        #print(len(query_feature_vector))\n",
    "\n",
    "        #--------------Passage processing\n",
    "        wordsWithoutPunctuation = re.split('\\W+', passage)\n",
    "        words = [word for word in wordsWithoutPunctuation if word not in stop_words and len(word) > 0]\n",
    "        if len(words) == 0:\n",
    "            words = wordsWithoutPunctuation\n",
    "\n",
    "        word_count = len(words)\n",
    "        remaining = max_passage_words - word_count  \n",
    "        if(remaining>0):\n",
    "            words += [\"zerovec\"]*remaining # Pad zero vecs if the word count is less than max_passage_words\n",
    "        words = words[:max_passage_words] # trim extra words\n",
    "        #create Passage Feature vector \n",
    "        passage_feature_vector = []\n",
    "        for word in words:\n",
    "            if(word in GloveEmbeddings):\n",
    "                passage_feature_vector += GloveEmbeddings[word]\n",
    "            else:\n",
    "                passage_feature_vector += GloveEmbeddings[\"zerovec\"]  #Add zerovec for OOV terms\n",
    "        #passage_feature_vector = np.array(passage_feature_vector.strip().split())\n",
    "        #print(len(passage_feature_vector))\n",
    "\n",
    "        #----------label Processing\n",
    "        if(not isEvaluation):\n",
    "            labelFeatureVector =[0,0]\n",
    "            labelFeatureVector[label] = 1\n",
    "\n",
    "            example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            'query': _floatList_feature(query_feature_vector),\n",
    "            'passage': _floatList_feature(passage_feature_vector),\n",
    "            'label': _int64_feature(labelFeatureVector)\n",
    "            }))\n",
    "\n",
    "            #fw.write(query_feature_vector + \",\" + passage_feature_vector + \",\" + str(label))\n",
    "        else:\n",
    "            #fw.write(query_feature_vector + \",\" + passage_feature_vector + \",\" + str(query_id))\n",
    "            example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            'query': _floatList_feature(query_feature_vector),\n",
    "            'passage': _floatList_feature(passage_feature_vector),\n",
    "            'query_id': _int64_feature(query_id),\n",
    "            'passage_id': _int64_feature([label])\n",
    "            }))\n",
    "\n",
    "        writer.write(example.SerializeToString())\n",
    "        currentRowNum += 1\n",
    "\n",
    "        #-------Close the file and update the variables\n",
    "        if currentRowNum == totalRowsInFile:\n",
    "            writer.close()\n",
    "            currentRowNum = 0\n",
    "            fileCounter += 1\n",
    "            writer = tf.python_io.TFRecordWriter(outputfile.format(str(fileCounter)),opts)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataset(dataFileName,trainFilename, validationFileName):\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    data = pd.read_csv(dataFileName, header = None, sep = \"\\t\")\n",
    "    train, test = train_test_split(data, test_size=0.10, random_state=42)\n",
    "    train.to_csv(trainFilename,index = False,header = None,sep = \"\\t\")\n",
    "    test.to_csv(validationFileName,index = False,header = None,sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadEmbeddings(\"./glove.6B.100d.txt\")\n",
    "#splitDataset(\"./data.tsv\",\"./trainData.tsv\",\"./ValidationData.tsv\")\n",
    "#removeStopWordsAndGenerateEmbeddings(\"./trainData.tsv\",\"./TrainData/trainEmbeddings_{}.tfrecords\",isEvaluation=False)\n",
    "#removeStopWordsAndGenerateEmbeddings(\"./ValidationData.tsv\",\"./ValidationData/validationEmbeddings_{}.tfrecords\",isEvaluation=False)\n",
    "removeStopWordsAndGenerateEmbeddings(\"./eval1_unlabelled.tsv\",\"./TestData/evalUnlabelledEmbeddings_{}.tfrecords\",isEvaluation=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
