{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import trange \n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "%run \"./utils.py\"\n",
    "\n",
    "params = {\n",
    "    \"max_query_words\": 12,\n",
    "    \"max_passage_words\": 50,\n",
    "    \"emb_dim\": 50,\n",
    "    \"BATCH_SIZE\": 100,\n",
    "    \"TotalTrainingdata\":4717692,\n",
    "    \"TotalValidationdata\":524188,\n",
    "    \"EPOCHS\" : 200,  #Total number of epochs to run\n",
    "    \"num_classes\": 2,\n",
    "    \"save_summary_steps\": 100,\n",
    "    \"TEST_BATCH_SIZE\": 1000,\n",
    "    \"SHUFFLE_BATCH_SIZE\": 10000,\n",
    "    \"WeightMultiplierPosClass\": 7.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_network(queryfeatures, passagefeatures,reuse = False):\n",
    "    \n",
    "    #queryfeatures = tf.placeholder(tf.float32,shape=(None,max_query_words * emb_dim))\n",
    "    #passagefeatures = tf.placeholder(tf.float32,shape=(None,max_passage_words * emb_dim))\n",
    "    \n",
    "    #global max_passage_words,max_query_words,emb_dim,num_classes\n",
    "\n",
    "    def conv2D(x,W,strides):\n",
    "        return tf.nn.conv2d(x,W,strides=strides,padding='VALID',name=\"conv2D\")\n",
    "\n",
    "    def maxPooling(x,k):\n",
    "        return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],padding='VALID',name = \"maxPool\")\n",
    "\n",
    "    def getWeight(shape):\n",
    "        return tf.get_variable(name = \"weight\",shape = shape, initializer=tf.initializers.truncated_normal(stddev= 0.1))\n",
    "\n",
    "    def getBias(shape):\n",
    "        return tf.get_variable(name = \"bias\",shape = shape, initializer=tf.initializers.constant(0.1))\n",
    "\n",
    "    with tf.variable_scope('query/convLayer1'):\n",
    "        #------Filter: [filter_height, filter_width, in_channels, out_channels]\n",
    "        with tf.variable_scope('shared',reuse=reuse):\n",
    "            weight = getWeight([2,5,1,64])\n",
    "            bias = getBias([64])\n",
    "        #------Input: [batch, in_height, in_width, in_channels]\n",
    "        #x_query = tf.reshape(queryfeatures,[-1,max_query_words,emb_dim,1], name=\"ReshapeInputOp\")  #input: ?,12,50,1\n",
    "        #------Stride [batch,height,width,channel]\n",
    "        convQuery1 = tf.nn.relu(conv2D(queryfeatures,weight,[1,1,1,1]) + bias)\n",
    "        convQuery1_max = maxPooling(convQuery1,k = 2)\n",
    "        print(convQuery1_max)\n",
    "\n",
    "    with tf.variable_scope('query/convLayer2'):\n",
    "        with tf.variable_scope('shared',reuse=reuse):\n",
    "            weight = getWeight([2,5,64,32])\n",
    "            bias = getBias([32])\n",
    "        convQuery2 = tf.nn.relu(conv2D(convQuery1_max,weight,[1,1,1,1]) + bias)\n",
    "        convQuery2_max = maxPooling(convQuery2,k = 2)\n",
    "        print(convQuery2_max)\n",
    "        \n",
    "    with tf.variable_scope('query/convLayer3'):\n",
    "        with tf.variable_scope('shared',reuse=reuse):\n",
    "            weight = getWeight([1,5,32,16])\n",
    "            bias = getBias([32])\n",
    "        convQuery3 = tf.nn.relu(conv2D(convQuery2_max,weight,[1,1,1,1]) + bias)\n",
    "        convQuery3_max = maxPooling(convQuery3,k = 2)\n",
    "        print(convQuery3_max)\n",
    "        \n",
    "    with tf.variable_scope('query/denseLayer3'):\n",
    "        with tf.variable_scope('shared',reuse=reuse):\n",
    "            weight = getWeight([8*16,10])\n",
    "            bias = getBias([10])\n",
    "        dense = tf.reshape(convQuery2_max,[-1,2*8*2])\n",
    "        denseQuery3 = tf.nn.relu(tf.matmul(dense,weight) + bias)\n",
    "        print(denseQuery3)\n",
    "\n",
    "    with tf.variable_scope('passage/convLayer1'):\n",
    "        with tf.variable_scope('shared',reuse=reuse):\n",
    "            weight = getWeight([2,5,1,64])\n",
    "            bias = getBias([64])\n",
    "        #x_passage = tf.reshape(passagefeatures,[-1,max_passage_words,emb_dim,1])\n",
    "        convPassage1 = tf.nn.relu(conv2D(passagefeatures,weight,[1,1,1,1]) + bias)\n",
    "        convPassage1_max = maxPooling(convPassage1,k = 3)\n",
    "        print(convPassage1_max)\n",
    "\n",
    "    with tf.variable_scope('passage/convLayer2'):\n",
    "        with tf.variable_scope('shared',reuse=reuse):\n",
    "            weight = getWeight([2,5,64,32])\n",
    "            bias = getBias([32])\n",
    "        convPassage2 = tf.nn.relu(conv2D(convPassage1_max,weight,[1,1,1,1]) + bias)\n",
    "        convPassage2_max = maxPooling(convPassage2,k = 3)\n",
    "        print(convPassage2_max)\n",
    "        \n",
    "    with tf.variable_scope('passage/convLayer3'):\n",
    "        with tf.variable_scope('shared',reuse=reuse):\n",
    "            weight = getWeight([2,2,32,16])\n",
    "            bias = getBias([16])\n",
    "        convPassage3 = tf.nn.relu(conv2D(convPassage2_max,weight,[1,1,1,1]) + bias)\n",
    "        convPassage3_max =  tf.nn.avg_pool(convPassage3, ksize=[1, 4, 1, 1], strides=[1, 1, 1, 1],padding='VALID',name = \"maxPool\")\n",
    "        print(convPassage3_max)\n",
    "        \n",
    "    with tf.variable_scope('mergeQueryPassage'):\n",
    "        convQuery3_max = tf.reshape(convQuery3_max, [-1,8*16])\n",
    "        convPassage3_max = tf.reshape(convPassage3_max, [-1,8*16])\n",
    "        mergeOp = tf.multiply(convQuery3_max,convPassage3_max, name = \"merge\")\n",
    "        print(mergeOp)\n",
    "        \n",
    "        with tf.variable_scope('shared',reuse=reuse):\n",
    "            weight = getWeight([8*16,10])\n",
    "            bias = getBias([10])\n",
    "        \n",
    "        mergeDense = tf.nn.relu(tf.matmul(mergeOp,weight) + bias)\n",
    "        print(mergeDense)\n",
    "\n",
    "    with tf.variable_scope('mergeQueryPassage/output'):\n",
    "        with tf.variable_scope('shared',reuse=reuse):\n",
    "            weight = getWeight([10,2])\n",
    "            bias = getBias([2])\n",
    "        \n",
    "        output = tf.nn.relu(tf.matmul(mergeDense,weight) + bias)\n",
    "\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----https://github.com/brightmart/text_classification/tree/master/a02_TextCNN\n",
    "def cnn_network(queryfeatures, passagefeatures,is_training, reuse = False):\n",
    "    filter_sizes = [2,3,4]\n",
    "    num_filters = 128\n",
    "    embed_size = 50\n",
    "    featureTypes = [\"query\",\"passage\"]\n",
    "    sequence_length = {\"query\":12,\"passage\":50}\n",
    "    def cnn_multiple_layers():\n",
    "        # 2.=====>loop each filter size. for each filter, do:convolution-pooling layer(a.create filters,b.conv,c.apply nolinearity,d.max-pooling)--->\n",
    "        # you can use:tf.nn.conv2d;tf.nn.relu;tf.nn.max_pool; feature shape is 4-d. feature is a new variable\n",
    "        dense_outputs = []\n",
    "        for _,featureType in enumerate(featureTypes):\n",
    "            #  =====> Select feature matrix based on whether its query or passage\n",
    "            featureMatrix = queryfeatures if featureType == \"query\" else passagefeatures\n",
    "            pooled_outputs = []\n",
    "            for i, filter_size in enumerate(filter_sizes):\n",
    "                with tf.variable_scope(featureType + '_cnn_multiple_layers' + \"convolution-pooling-%s\" % filter_size,reuse = reuse):\n",
    "                    # 1) CNN->BN->relu\n",
    "                    filter = tf.get_variable(\"filter-%s\" % filter_size,[filter_size, embed_size, 1, num_filters],initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "                    conv = tf.nn.conv2d(featureMatrix, filter, strides=[1, 1, 1, 1],padding=\"VALID\",name=\"conv\")  # shape:[batch_size,sequence_length - filter_size + 1,1,num_filters]\n",
    "                    conv = tf.contrib.layers.batch_norm(conv, is_training=is_training, scope='cnn1')\n",
    "                    #print(i, \"conv1:\", conv)\n",
    "                    b = tf.get_variable(\"b-%s\" % filter_size, [num_filters])  # ADD 2017-06-09\n",
    "                    h = tf.nn.relu(tf.nn.bias_add(conv, b),\"relu\")  # shape:[batch_size,sequence_length-1,1,num_filters]. tf.nn.bias_add:adds `bias` to `value`\n",
    "\n",
    "                    # 3. Max-pooling\n",
    "                    pooling_max = tf.nn.max_pool(h, ksize=[1,sequence_length[featureType] - filter_size + 1, 1, 1],strides=[1, 1, 1, 1], padding='VALID', name=\"pool\")\n",
    "                    # pooling_avg=tf.squeeze(tf.reduce_mean(h,axis=1)) #[batch_size,num_filters]\n",
    "                    #print(i, \"pooling:\", pooling_max)\n",
    "                    # pooling=tf.concat([pooling_max,pooling_avg],axis=1) #[batch_size,num_filters*2]\n",
    "                    pooled_outputs.append(pooling_max)  # h:[batch_size,sequence_length,1,num_filters]\n",
    "            # concat\n",
    "            h = tf.concat(pooled_outputs, axis=3)  # [batch_size,num_filters_total]\n",
    "            #print(\"h.concat:\", h)\n",
    "            \n",
    "            with tf.name_scope(\"dropout\"):\n",
    "                h = tf.nn.dropout(h,keep_prob=1.0)  # [batch_size,sequence_length - filter_size + 1,num_filters]\n",
    "            #print(h.shape)\n",
    "            h = tf.reshape(h,[-1,len(filter_sizes) * num_filters])\n",
    "            #print(\"Reshaped: \",h.shape)\n",
    "            h = tf.layers.dense(h, 100, activation=tf.nn.relu, use_bias=True,reuse = reuse, name = featureType + \"_Reshape\")\n",
    "            \n",
    "            #print(\"Dense:\", h)\n",
    "            dense_outputs.append(h)\n",
    "        \n",
    "        predictionsOp = tf.multiply(dense_outputs[0],dense_outputs[1], name = \"merge\")\n",
    "        \n",
    "        with tf.variable_scope(\"mergeLayer\", reuse = reuse):\n",
    "            predictions = tf.layers.dense(predictionsOp,units = 2,use_bias = True,activation=tf.nn.relu,reuse=reuse,name=\"predictions\")\n",
    "        \n",
    "        return predictions\n",
    "    return cnn_multiple_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelTest_fn(mode,embeddingsFile,params):    \n",
    "    #--------Hyper parameters:\n",
    "    max_query_words = params[\"max_query_words\"]\n",
    "    max_passage_words = params[\"max_passage_words\"]\n",
    "    emb_dim = params[\"emb_dim\"]\n",
    "    BATCH_SIZE = params[\"TEST_BATCH_SIZE\"]\n",
    "    EPOCHS = params[\"EPOCHS\"]\n",
    "    num_classes = params[\"num_classes\"]\n",
    "    \n",
    "    is_training = (mode == \"train\")\n",
    "    \n",
    "    def testDSParser(example_proto):\n",
    "        features = {\"query\": tf.FixedLenFeature((max_query_words,emb_dim,1), tf.float32),\n",
    "                  \"passage\": tf.FixedLenFeature((max_passage_words,emb_dim,1), tf.float32),\n",
    "                  \"query_id\": tf.FixedLenFeature((1), tf.int64),\n",
    "                  \"passage_id\": tf.FixedLenFeature((1), tf.int64)}\n",
    "        parsed_features = tf.parse_single_example(example_proto, features)\n",
    "        return parsed_features[\"query\"], parsed_features[\"passage\"],parsed_features[\"query_id\"],parsed_features[\"passage_id\"]\n",
    "    \n",
    "    def getDatasetIterator(fileName,batch_size,mode):\n",
    "        dataset = tf.data.TFRecordDataset(filenames = fileName, compression_type=\"ZLIB\")\n",
    "        #------Follow this order: map -> prefetch -> batch\n",
    "        dataset = dataset.map(testDSParser)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        \n",
    "        iterator = dataset.make_initializable_iterator()\n",
    "        return iterator\n",
    "    \n",
    "    iterator = getDatasetIterator(embeddingsFile,BATCH_SIZE,mode)\n",
    "    \n",
    "    queryfeatures,passagefeatures,query_id,passage_id = iterator.get_next()\n",
    "    \n",
    "    model_spec =     {\n",
    "        'queryfeatures': queryfeatures,\n",
    "        'passagefeatures': passagefeatures,\n",
    "        'iterator_init_op': iterator.initializer,\n",
    "        \"query_id\":query_id,\n",
    "        \"passage_id\": passage_id\n",
    "    }\n",
    "    \n",
    "    with tf.variable_scope('model'):\n",
    "        y_conv = cnn_network(queryfeatures,passagefeatures,is_training,reuse = False)\n",
    "    \n",
    "    # -----------------------------------------------------------\n",
    "    # MODEL SPECIFICATION\n",
    "    # Create the model specification and return it\n",
    "    # It contains nodes or operations in the graph that will be used for training and evaluation\n",
    "    variable_init_op = tf.group(*[tf.global_variables_initializer()])\n",
    "    model_spec['variable_init_op'] = variable_init_op\n",
    "    model_spec[\"predictions\"] = y_conv\n",
    "    model_spec['summary_op'] = tf.summary.merge_all()\n",
    "\n",
    "    \n",
    "    return model_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(mode,embeddingsFile,params):    \n",
    "    #--------Hyper parameters:\n",
    "    max_query_words = params[\"max_query_words\"]\n",
    "    max_passage_words = params[\"max_passage_words\"]\n",
    "    emb_dim = params[\"emb_dim\"]\n",
    "    BATCH_SIZE = params[\"BATCH_SIZE\"]\n",
    "    SHUFFLE_BATCH_SIZE = params[\"SHUFFLE_BATCH_SIZE\"]\n",
    "    EPOCHS = params[\"EPOCHS\"]\n",
    "    num_classes = params[\"num_classes\"]\n",
    "    \n",
    "    is_training = (mode == \"train\")\n",
    "    \n",
    "    def parseEachRecord(record):\n",
    "        features = {\"query\": tf.FixedLenFeature((max_query_words,emb_dim,1), tf.float32),\n",
    "                  \"passage\": tf.FixedLenFeature((max_passage_words,emb_dim,1), tf.float32),\n",
    "                  \"label\": tf.FixedLenFeature((num_classes), tf.int64)}\n",
    "        parsed_features = tf.parse_single_example(record, features)\n",
    "        return parsed_features[\"query\"], parsed_features[\"passage\"],parsed_features[\"label\"]\n",
    "\n",
    "    def parser(fileName):\n",
    "        dataset = tf.data.TFRecordDataset(filenames = fileName, compression_type=\"ZLIB\")\n",
    "        parsed_features = dataset.map(parseEachRecord)\n",
    "        return parsed_features\n",
    "    \n",
    "    def getDatasetIterator(folderPath,batch_size,SHUFFLE_BATCH_SIZE,mode):\n",
    "        files = [os.path.join(folderPath, f) for f in os.listdir(folderPath)]\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(files).shuffle(buffer_size = len(files))\n",
    "        #dataset = tf.data.TFRecordDataset(filenames = fileName, compression_type=\"ZLIB\").shuffle(buffer_size = len(files))\n",
    "        #------Follow this order: map -> prefetch -> batch\n",
    "        dataset = dataset.flat_map(parser)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        dataset = dataset.prefetch(1)\n",
    "        iterator = dataset.make_initializable_iterator()\n",
    "        return iterator\n",
    "    \n",
    "   \n",
    "    iterator = getDatasetIterator(embeddingsFile,BATCH_SIZE,SHUFFLE_BATCH_SIZE,mode)\n",
    "    \n",
    "    queryfeatures,passagefeatures,y = iterator.get_next()\n",
    "    \n",
    "    model_spec =     {\n",
    "        'queryfeatures': queryfeatures[0],\n",
    "        'passagefeatures': passagefeatures[0],\n",
    "        'iterator_init_op': iterator.initializer,\n",
    "        \"y\":y[0]\n",
    "    }\n",
    "    \n",
    "    with tf.variable_scope('model'):\n",
    "        y_conv = cnn_network(queryfeatures,passagefeatures,is_training,reuse = not is_training)\n",
    "    \n",
    "    with tf.variable_scope('lossPerBatch'):\n",
    "        tmpMultiplier = params[\"WeightMultiplierPosClass\"]\n",
    "        weights = tf.multiply(tmpMultiplier, tf.cast(tf.argmax(y,-1),tf.float32)) + 1\n",
    "        cross_entropy = tf.reduce_mean(tf.losses.softmax_cross_entropy(onehot_labels=y,logits=y_conv,weights = weights))\n",
    "        #cross_entropy = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(targets=tf.cast(y,tf.float32),logits=y_conv,pos_weight = 9.0))\n",
    "        tf.summary.scalar('lossPerBatch', cross_entropy)\n",
    "\n",
    "    if is_training:\n",
    "        with tf.name_scope('AdamOptim'):\n",
    "            global_step = tf.train.get_or_create_global_step()\n",
    "            train_step = tf.train.AdamOptimizer(1e-4).minimize(loss = cross_entropy,global_step=global_step)\n",
    "\n",
    "    correct_pred = tf.equal(tf.argmax(y_conv,1),tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "    tf.summary.scalar('accuracyPerBatch', accuracy)\n",
    "    \n",
    "    \n",
    "    # -----------------------------------------------------------\n",
    "    # METRICS AND SUMMARIES\n",
    "    # Metrics for evaluation using tf.metrics (average over whole dataset)\n",
    "    with tf.variable_scope(\"metrics\"):\n",
    "        metrics = {\n",
    "            'accuracy': tf.metrics.accuracy(labels=tf.argmax(y,-1), predictions=tf.argmax(y_conv,-1)),\n",
    "            'loss': tf.metrics.mean(cross_entropy),\n",
    "            'auc':tf.metrics.auc(labels=tf.argmax(y,-1), predictions=tf.argmax(y_conv,-1))\n",
    "        }\n",
    "    \n",
    "    accuracyOverall,_ = metrics[\"accuracy\"]\n",
    "    lossOverall,_ = metrics[\"loss\"]\n",
    "    aucOVerall,_ = metrics[\"auc\"]\n",
    "    \n",
    "    tf.summary.scalar('accuracy', accuracyOverall)\n",
    "    tf.summary.scalar('loss',lossOverall )\n",
    "    tf.summary.scalar('auc', aucOVerall)\n",
    "    \n",
    "    # Group the update ops for the tf.metrics\n",
    "    update_metrics_op = tf.group(*[op for _, op in metrics.values()])\n",
    "\n",
    "    # Get the op to reset the local variables used in tf.metrics\n",
    "    metric_variables = tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES, scope=\"metrics\")\n",
    "    metrics_init_op = tf.variables_initializer(metric_variables)\n",
    "    \n",
    "    \n",
    "    # -----------------------------------------------------------\n",
    "    # MODEL SPECIFICATION\n",
    "    # Create the model specification and return it\n",
    "    # It contains nodes or operations in the graph that will be used for training and evaluation\n",
    "    variable_init_op = tf.group(*[tf.global_variables_initializer()])\n",
    "    model_spec['variable_init_op'] = variable_init_op\n",
    "    model_spec[\"predictions\"] = y_conv\n",
    "    model_spec['loss'] = cross_entropy\n",
    "    model_spec['accuracy'] = accuracy\n",
    "    model_spec['metrics_init_op'] = metrics_init_op\n",
    "    model_spec['metrics'] = metrics\n",
    "    model_spec['update_metrics'] = update_metrics_op\n",
    "    model_spec['summary_op'] = tf.summary.merge_all()\n",
    "\n",
    "    if is_training:\n",
    "        model_spec['train_op'] = train_step\n",
    "    \n",
    "    return model_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sess(sess, model_spec, num_steps, writer, params):\n",
    "    \"\"\"Train the model on `num_steps` batches\n",
    "\n",
    "    Args:\n",
    "        sess: (tf.Session) current session\n",
    "        model_spec: (dict) contains the graph operations or nodes needed for training\n",
    "        num_steps: (int) train for this number of batches\n",
    "        writer: (tf.summary.FileWriter) writer for summaries\n",
    "        params: (Params) hyperparameters\n",
    "    \"\"\"\n",
    "    # Get relevant graph operations or nodes needed for training\n",
    "    loss = model_spec['loss']\n",
    "    train_op = model_spec['train_op']\n",
    "    update_metrics = model_spec['update_metrics']\n",
    "    metrics = model_spec['metrics']\n",
    "    summary_op = model_spec['summary_op']\n",
    "    y_conv = model_spec[\"predictions\"]\n",
    "    global_step = tf.train.get_global_step()\n",
    "\n",
    "    # Load the training dataset into the pipeline and initialize the metrics local variables\n",
    "    sess.run(model_spec['iterator_init_op'])\n",
    "    sess.run(model_spec['metrics_init_op'])\n",
    "\n",
    "    # Use tqdm for progress bar\n",
    "    t = trange(num_steps)\n",
    "    for i in t:\n",
    "        # Evaluate summaries for tensorboard only once in a while\n",
    "        if i % params[\"save_summary_steps\"] == 0:\n",
    "            # Perform a mini-batch update\n",
    "            _, _, loss_val, summ, global_step_val = sess.run([train_op, update_metrics, loss,summary_op, global_step])\n",
    "            # Write summaries for tensorboard\n",
    "            #print(\"Global Step: \",global_step_val)\n",
    "            writer.add_summary(summ, global_step_val)\n",
    "        else:\n",
    "            _, _, loss_val = sess.run([train_op, update_metrics, loss])\n",
    "        # Log the loss in the tqdm progress bar\n",
    "        t.set_postfix(loss='{:05.3f}'.format(loss_val))\n",
    "        #print(\"Predictions\",sess.run([y_conv[0:10,]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sess(sess, model_spec, num_steps, writer=None, params=None):\n",
    "    \"\"\"Train the model on `num_steps` batches.\n",
    "\n",
    "    Args:\n",
    "        sess: (tf.Session) current session\n",
    "        model_spec: (dict) contains the graph operations or nodes needed for training\n",
    "        num_steps: (int) train for this number of batches\n",
    "        writer: (tf.summary.FileWriter) writer for summaries. Is None if we don't log anything\n",
    "        params: (Params) hyperparameters\n",
    "    \"\"\"\n",
    "    update_metrics = model_spec['update_metrics']\n",
    "    eval_metrics = model_spec['metrics']\n",
    "    global_step = tf.train.get_global_step()\n",
    "\n",
    "    # Load the evaluation dataset into the pipeline and initialize the metrics init op\n",
    "    sess.run(model_spec['iterator_init_op'])\n",
    "    sess.run(model_spec['metrics_init_op'])\n",
    "\n",
    "    # compute metrics over the dataset\n",
    "    for _ in range(num_steps):\n",
    "        sess.run(update_metrics)\n",
    "\n",
    "    # Get the values of the metrics\n",
    "    metrics_values = {k: v[0] for k, v in eval_metrics.items()}\n",
    "    metrics_val = sess.run(metrics_values)\n",
    "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_val.items())\n",
    "    logging.info(\"- Eval metrics: \" + metrics_string)\n",
    "\n",
    "    # Add summaries manually to writer at global_step_val\n",
    "    if writer is not None:\n",
    "        global_step_val = sess.run(global_step)\n",
    "        for tag, val in metrics_val.items():\n",
    "            summ = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=val)])\n",
    "            writer.add_summary(summ, global_step_val)\n",
    "\n",
    "    return metrics_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_spec, model_dir, params, restore_from):\n",
    "    import pandas as pd\n",
    "    \"\"\"Evaluate the model\n",
    "\n",
    "    Args:\n",
    "        model_spec: (dict) contains the graph operations or nodes needed for evaluation\n",
    "        model_dir: (string) directory containing config, weights and log\n",
    "        params: (Params) contains hyperparameters of the model.\n",
    "                Must define: num_epochs, train_size, batch_size, eval_size, save_summary_steps\n",
    "        restore_from: (string) directory or file containing weights to restore the graph\n",
    "    \"\"\"\n",
    "    # Initialize tf.Saver\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize the lookup table\n",
    "        #sess.run(model_spec['variable_init_op'])\n",
    "        # Reload weights from the weights subdirectory\n",
    "        save_path = os.path.join(model_dir, restore_from)\n",
    "        if os.path.isdir(save_path):\n",
    "            save_path = tf.train.latest_checkpoint(save_path)\n",
    "        saver.restore(sess, save_path)\n",
    "        \n",
    "        sess.run(model_spec['iterator_init_op'])\n",
    "        totalBatches = (104170//params[\"TEST_BATCH_SIZE\"]) + 1\n",
    "        \n",
    "        for index in range(totalBatches):        \n",
    "            predictions,query_id,passage_id = sess.run([model_spec[\"predictions\"],model_spec[\"query_id\"],model_spec[\"passage_id\"]])\n",
    "            \n",
    "            tmp = pd.DataFrame({\"query_id\":query_id[:,0],\"passage_id\":passage_id[:,0],\"predictions\":predictions[:,1]})\n",
    "            df = pd.concat([df,tmp],axis = 0)\n",
    "            #print(query_id[0:100])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(train_model_spec, eval_model_spec, model_dir, params):\n",
    "    \"\"\"Train the model and evaluate every epoch.\n",
    "\n",
    "    Args:\n",
    "        train_model_spec: (dict) contains the graph operations or nodes needed for training\n",
    "        eval_model_spec: (dict) contains the graph operations or nodes needed for evaluation\n",
    "        model_dir: (string) directory containing config, weights and log\n",
    "        params: (Params) contains hyperparameters of the model.\n",
    "                Must define: num_epochs, train_size, batch_size, eval_size, save_summary_steps\n",
    "        restore_from: (string) directory or file containing weights to restore the graph\n",
    "    \"\"\"\n",
    "    \n",
    "    max_query_words = params[\"max_query_words\"]\n",
    "    max_passage_words = params[\"max_passage_words\"]\n",
    "    emb_dim = params[\"emb_dim\"]\n",
    "    BATCH_SIZE = params[\"BATCH_SIZE\"]\n",
    "    EPOCHS = params[\"EPOCHS\"]\n",
    "    TotalTrainingdata = params[\"TotalTrainingdata\"]\n",
    "    num_classes = params[\"num_classes\"]\n",
    "    \n",
    "    # Initialize tf.Saver instances to save weights during training\n",
    "    last_saver = tf.train.Saver() # will keep last 5 epochs\n",
    "    best_saver = tf.train.Saver(max_to_keep=1)  # only keep 1 best checkpoint (best on eval)\n",
    "    begin_at_epoch = 0\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # Initialize model variables\n",
    "        sess.run(train_model_spec['variable_init_op'])\n",
    "\n",
    "        # For tensorboard (takes care of writing summaries to files)\n",
    "        train_writer = tf.summary.FileWriter(os.path.join(model_dir, 'train_summaries'), sess.graph)\n",
    "        eval_writer = tf.summary.FileWriter(os.path.join(model_dir, 'eval_summaries'), sess.graph)\n",
    "\n",
    "        best_eval_acc = 0.0\n",
    "        for epoch in range(EPOCHS):\n",
    "            # Run one epoch\n",
    "            logging.info(\"Epoch {}/{}\".format(epoch + 1, EPOCHS))\n",
    "            \n",
    "            numSteps = TotalTrainingdata // BATCH_SIZE\n",
    "            train_sess(sess, train_model_spec, numSteps, train_writer, params)\n",
    "\n",
    "            # Save weights\n",
    "            last_save_path = os.path.join(model_dir, 'last_weights', 'after-epoch')\n",
    "            last_saver.save(sess, last_save_path, global_step=epoch + 1)\n",
    "\n",
    "            metrics = evaluate_sess(sess, eval_model_spec, 1, eval_writer)\n",
    "\n",
    "            # If best_eval, best_save_path\n",
    "            eval_acc = metrics['auc']\n",
    "            if eval_acc >= best_eval_acc:\n",
    "                # Store new best accuracy\n",
    "                best_eval_acc = eval_acc\n",
    "                # Save weights\n",
    "                best_save_path = os.path.join(model_dir, 'best_weights', 'after-epoch')\n",
    "                best_save_path = best_saver.save(sess, best_save_path, global_step=epoch + 1)\n",
    "                logging.info(\"- Found new best accuracy, saving in {}\".format(best_save_path))\n",
    "                # Save best eval metrics in a json file in the model directory\n",
    "                best_json_path = os.path.join(model_dir, \"metrics_eval_best_weights.json\")\n",
    "                save_dict_to_json(metrics, best_json_path)\n",
    "\n",
    "            # Save latest eval metrics in a json file in the model directory\n",
    "            last_json_path = os.path.join(model_dir, \"metrics_eval_last_weights.json\")\n",
    "            save_dict_to_json(metrics, last_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 47176/47176 [26:19<00:00, 30.00it/s, loss=1.033]\n",
      "100%|███████████████████████████████████████████████████████████████████| 47176/47176 [25:41<00:00, 30.60it/s, loss=1.324]\n",
      "100%|███████████████████████████████████████████████████████████████████| 47176/47176 [25:37<00:00, 30.69it/s, loss=1.178]\n",
      "100%|███████████████████████████████████████████████████████████████████| 47176/47176 [25:12<00:00, 31.18it/s, loss=1.227]\n",
      "100%|███████████████████████████████████████████████████████████████████| 47176/47176 [25:18<00:00, 31.08it/s, loss=1.130]\n",
      "100%|███████████████████████████████████████████████████████████████████| 47176/47176 [25:26<00:00, 30.91it/s, loss=1.081]\n",
      "100%|███████████████████████████████████████████████████████████████████| 47176/47176 [25:13<00:00, 31.16it/s, loss=1.324]\n",
      "100%|███████████████████████████████████████████████████████████████████| 47176/47176 [25:41<00:00, 30.60it/s, loss=1.130]\n",
      "100%|███████████████████████████████████████████████████████████████████| 47176/47176 [25:27<00:00, 30.89it/s, loss=1.033]\n",
      "100%|███████████████████████████████████████████████████████████████████| 47176/47176 [25:29<00:00, 30.85it/s, loss=1.033]\n",
      "100%|███████████████████████████████████████████████████████████████████| 47176/47176 [25:27<00:00, 30.89it/s, loss=1.081]\n",
      "100%|███████████████████████████████████████████████████████████████████| 47176/47176 [25:29<00:00, 30.29it/s, loss=1.324]\n",
      " 78%|████████████████████████████████████████████████████▏              | 36784/47176 [19:52<05:47, 29.92it/s, loss=1.372]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-a5c8e4564b56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0meval_model_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"eval\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"./ValidationData\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Starting training for {} epoch(s)\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"EPOCHS\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_model_spec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_model_spec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"./ModelLogs\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-e4db2a154f9b>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(train_model_spec, eval_model_spec, model_dir, params)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mnumSteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTotalTrainingdata\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0mtrain_sess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_model_spec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumSteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_writer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[1;31m# Save weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-43b759fda040>\u001b[0m in \u001b[0;36mtrain_sess\u001b[1;34m(sess, model_spec, num_steps, writer, params)\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate_metrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;31m# Log the loss in the tqdm progress bar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'{:05.3f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[1;31m#print(\"Predictions\",sess.run([y_conv[0:10,]]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tqdm\\_tqdm.py\u001b[0m in \u001b[0;36mset_postfix\u001b[1;34m(self, ordered_dict, refresh, **kwargs)\u001b[0m\n\u001b[0;32m   1229\u001b[0m                                  for key in postfix.keys())\n\u001b[0;32m   1230\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1231\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1233\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mset_postfix_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tqdm\\_tqdm.py\u001b[0m in \u001b[0;36mrefresh\u001b[1;34m(self, nolock)\u001b[0m\n\u001b[0;32m   1269\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1270\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1271\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1272\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1273\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnolock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tqdm\\_tqdm.py\u001b[0m in \u001b[0;36mprint_status\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mprint_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[0mlen_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m             \u001b[0mfp_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\r'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_len\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m             \u001b[0mlast_len\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tqdm\\_tqdm.py\u001b[0m in \u001b[0;36mfp_write\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfp_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m             \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m             \u001b[0mfp_flush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\colorama\\ansitowin32.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__convertor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\colorama\\ansitowin32.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_and_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\colorama\\ansitowin32.py\u001b[0m in \u001b[0;36mwrite_and_convert\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_ansi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[0mcursor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_plain_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\colorama\\ansitowin32.py\u001b[0m in \u001b[0;36mwrite_plain_text\u001b[1;34m(self, text, start, end)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mflush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[1;31m# and give a timeout to avoid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m                     \u001b[1;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m                     \u001b[1;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#-----------------Main function for training\n",
    "\n",
    "tf.reset_default_graph()\n",
    "train_model_spec = model_fn(\"train\",\"./TrainData\",params)\n",
    "eval_model_spec = model_fn(\"eval\",\"./ValidationData\",params)\n",
    "logging.info(\"Starting training for {} epoch(s)\".format(params[\"EPOCHS\"]))\n",
    "train_and_evaluate(train_model_spec, eval_model_spec, \"./ModelLogs\", params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./ModelLogs\\best_weights\\after-epoch-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./ModelLogs\\best_weights\\after-epoch-10\n"
     ]
    }
   ],
   "source": [
    "#-----------------Main function for testing\n",
    "\n",
    "tf.reset_default_graph()\n",
    "eval_model_spec = modelTest_fn(\"test\",\"./evalUnlabelledEmbeddings.tfrecords\",params)\n",
    "logging.info(\"Starting training for {} epoch(s)\".format(params[\"EPOCHS\"]))\n",
    "df = evaluate(eval_model_spec, \"./ModelLogs\", params,\"best_weights\")\n",
    "\n",
    "import csv\n",
    "tmp = (df.groupby('query_id')['predictions']\n",
    "       .apply(lambda x: \"\\t\".join([format(val, \"0.2f\") for val in x]))\n",
    "       .reset_index())\n",
    "\n",
    "tmp.to_csv(\"./answer.tsv\",index=False,sep= \"\\t\",header=None,quoting=csv.QUOTE_NONE,quotechar=\"\",  escapechar=\"\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head(100)\n",
    "#map(str,x)\n",
    "import csv\n",
    "tmp = (df.groupby('query_id')['predictions']\n",
    "       .apply(lambda x: \"\\t\".join([format(val, \"0.2f\") for val in x]))\n",
    "       .reset_index())\n",
    "\n",
    "tmp.to_csv(\"./answer.tsv\",index=False,sep= \"\\t\",header=None,quoting=csv.QUOTE_NONE,quotechar=\"\",  escapechar=\"\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"query_id\"] == 89][\"predictions\"].apply(lambda x: \"\\t\".join(format(x, \"0.2f\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
