{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import trange \n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "%run \"./utils.py\"\n",
    "\n",
    "params = {\n",
    "    \"max_query_words\": 12,\n",
    "    \"max_passage_words\": 50,\n",
    "    \"emb_dim\": 50,\n",
    "    \"BATCH_SIZE\": 100,\n",
    "    \"TotalTrainingdata\":4717692,\n",
    "    \"TotalValidationdata\":524188,\n",
    "    \"EPOCHS\" : 200,  #Total number of epochs to run\n",
    "    \"num_classes\": 2,\n",
    "    \"save_summary_steps\": 100,\n",
    "    \"TEST_BATCH_SIZE\": 1000,\n",
    "    \"SHUFFLE_BATCH_SIZE\": 10000,\n",
    "    \"WeightMultiplierPosClass\": 7.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_network(queryfeatures, passagefeatures,reuse = False):\n",
    "    \n",
    "    #queryfeatures = tf.placeholder(tf.float32,shape=(None,max_query_words * emb_dim))\n",
    "    #passagefeatures = tf.placeholder(tf.float32,shape=(None,max_passage_words * emb_dim))\n",
    "    \n",
    "    #global max_passage_words,max_query_words,emb_dim,num_classes\n",
    "\n",
    "    def conv2D(x,W,strides):\n",
    "        return tf.nn.conv2d(x,W,strides=strides,padding='VALID',name=\"conv2D\")\n",
    "\n",
    "    def maxPooling(x,k):\n",
    "        return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],padding='VALID',name = \"maxPool\")\n",
    "\n",
    "    def getWeight(shape):\n",
    "        return tf.get_variable(name = \"weight\",shape = shape, initializer=tf.initializers.truncated_normal(stddev= 0.1))\n",
    "\n",
    "    def getBias(shape):\n",
    "        return tf.get_variable(name = \"bias\",shape = shape, initializer=tf.initializers.constant(0.1))\n",
    "\n",
    "    with tf.variable_scope('query/convLayer1'):\n",
    "        #------Filter: [filter_height, filter_width, in_channels, out_channels]\n",
    "        with tf.variable_scope('shared',reuse=reuse):\n",
    "            weight = getWeight([2,5,1,4])\n",
    "            bias = getBias([4])\n",
    "        #------Input: [batch, in_height, in_width, in_channels]\n",
    "        #x_query = tf.reshape(queryfeatures,[-1,max_query_words,emb_dim,1], name=\"ReshapeInputOp\")  #input: ?,12,50,1\n",
    "        #------Stride [batch,height,width,channel]\n",
    "        convQuery1 = tf.nn.relu(conv2D(queryfeatures,weight,[1,1,1,1]) + bias)\n",
    "        convQuery1_max = maxPooling(convQuery1,k = 2)\n",
    "\n",
    "    with tf.variable_scope('query/convLayer2'):\n",
    "        with tf.variable_scope('shared',reuse=reuse):\n",
    "            weight = getWeight([2,4,4,2])\n",
    "            bias = getBias([2])\n",
    "        convQuery2 = tf.nn.relu(conv2D(convQuery1_max,weight,[1,1,1,1]) + bias)\n",
    "        convQuery2_max = maxPooling(convQuery2,k = 2)\n",
    "        print(convQuery2_max)\n",
    "\n",
    "    with tf.variable_scope('query/denseLayer3'):\n",
    "        with tf.variable_scope('shared',reuse=reuse):\n",
    "            weight = getWeight([2*8*2,10])\n",
    "            bias = getBias([10])\n",
    "        dense = tf.reshape(convQuery2_max,[-1,2*8*2])\n",
    "        denseQuery3 = tf.nn.relu(tf.matmul(dense,weight) + bias)\n",
    "\n",
    "    with tf.variable_scope('passage/convLayer1'):\n",
    "        with tf.variable_scope('shared',reuse=reuse):\n",
    "            weight = getWeight([5,10,1,4])\n",
    "            bias = getBias([4])\n",
    "        #x_passage = tf.reshape(passagefeatures,[-1,max_passage_words,emb_dim,1])\n",
    "        convPassage1 = tf.nn.relu(conv2D(passagefeatures,weight,[1,1,1,1]) + bias)\n",
    "        convPassage1_max = maxPooling(convPassage1,k = 2)\n",
    "\n",
    "\n",
    "    with tf.variable_scope('passage/convLayer2'):\n",
    "        with tf.variable_scope('shared',reuse=reuse):\n",
    "            weight = getWeight([3,10,4,4])\n",
    "            bias = getBias([4])\n",
    "        convPassage2 = tf.nn.relu(conv2D(convPassage1_max,weight,[1,1,1,1]) + bias)\n",
    "        convPassage2_max = maxPooling(convPassage2,k = 2)\n",
    "\n",
    "    with tf.variable_scope('passage/denseLayer3'):\n",
    "        with tf.variable_scope('shared',reuse=reuse):\n",
    "            weight = getWeight([10*5*4,10])\n",
    "            bias = getBias([10])\n",
    "        densePassage = tf.reshape(convPassage2_max,[-1,10*5*4])\n",
    "        densePassage3 = tf.nn.relu(tf.matmul(densePassage,weight) + bias)\n",
    "\n",
    "    with tf.variable_scope('mergeQueryPassage'):\n",
    "        with tf.variable_scope('shared',reuse=reuse):\n",
    "            weight = getWeight([10,2])\n",
    "            bias = getBias([2])\n",
    "        \n",
    "        mergeOp = tf.multiply(denseQuery3,densePassage3, name = \"merge\")\n",
    "        mergeDense = tf.nn.relu(tf.matmul(mergeOp,weight) + bias)\n",
    "\n",
    "    return mergeDense\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelTest_fn(mode,embeddingsFile,params):    \n",
    "    #--------Hyper parameters:\n",
    "    max_query_words = params[\"max_query_words\"]\n",
    "    max_passage_words = params[\"max_passage_words\"]\n",
    "    emb_dim = params[\"emb_dim\"]\n",
    "    BATCH_SIZE = params[\"TEST_BATCH_SIZE\"]\n",
    "    EPOCHS = params[\"EPOCHS\"]\n",
    "    num_classes = params[\"num_classes\"]\n",
    "    \n",
    "    is_training = (mode == \"train\")\n",
    "    \n",
    "    def testDSParser(example_proto):\n",
    "        features = {\"query\": tf.FixedLenFeature((max_query_words,emb_dim,1), tf.float32),\n",
    "                  \"passage\": tf.FixedLenFeature((max_passage_words,emb_dim,1), tf.float32),\n",
    "                  \"query_id\": tf.FixedLenFeature((1), tf.int64),\n",
    "                  \"passage_id\": tf.FixedLenFeature((1), tf.int64)}\n",
    "        parsed_features = tf.parse_single_example(example_proto, features)\n",
    "        return parsed_features[\"query\"], parsed_features[\"passage\"],parsed_features[\"query_id\"],parsed_features[\"passage_id\"]\n",
    "    \n",
    "    def getDatasetIterator(fileName,batch_size,mode):\n",
    "        dataset = tf.data.TFRecordDataset(filenames = fileName, compression_type=\"ZLIB\")\n",
    "        #------Follow this order: map -> prefetch -> batch\n",
    "        dataset = dataset.map(testDSParser)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        \n",
    "        iterator = dataset.make_initializable_iterator()\n",
    "        return iterator\n",
    "    \n",
    "    iterator = getDatasetIterator(embeddingsFile,BATCH_SIZE,mode)\n",
    "    \n",
    "    queryfeatures,passagefeatures,query_id,passage_id = iterator.get_next()\n",
    "    \n",
    "    model_spec =     {\n",
    "        'queryfeatures': queryfeatures,\n",
    "        'passagefeatures': passagefeatures,\n",
    "        'iterator_init_op': iterator.initializer,\n",
    "        \"query_id\":query_id,\n",
    "        \"passage_id\": passage_id\n",
    "    }\n",
    "    \n",
    "    with tf.variable_scope('model'):\n",
    "        y_conv = cnn_network(queryfeatures,passagefeatures,reuse = False)\n",
    "    \n",
    "    # -----------------------------------------------------------\n",
    "    # MODEL SPECIFICATION\n",
    "    # Create the model specification and return it\n",
    "    # It contains nodes or operations in the graph that will be used for training and evaluation\n",
    "    variable_init_op = tf.group(*[tf.global_variables_initializer()])\n",
    "    model_spec['variable_init_op'] = variable_init_op\n",
    "    model_spec[\"predictions\"] = y_conv\n",
    "    model_spec['summary_op'] = tf.summary.merge_all()\n",
    "\n",
    "    \n",
    "    return model_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(mode,embeddingsFile,params):    \n",
    "    #--------Hyper parameters:\n",
    "    max_query_words = params[\"max_query_words\"]\n",
    "    max_passage_words = params[\"max_passage_words\"]\n",
    "    emb_dim = params[\"emb_dim\"]\n",
    "    BATCH_SIZE = params[\"BATCH_SIZE\"]\n",
    "    SHUFFLE_BATCH_SIZE = params[\"SHUFFLE_BATCH_SIZE\"]\n",
    "    EPOCHS = params[\"EPOCHS\"]\n",
    "    num_classes = params[\"num_classes\"]\n",
    "    \n",
    "    is_training = (mode == \"train\")\n",
    "    \n",
    "    def parseEachRecord(record):\n",
    "        features = {\"query\": tf.FixedLenFeature((max_query_words,emb_dim,1), tf.float32),\n",
    "                  \"passage\": tf.FixedLenFeature((max_passage_words,emb_dim,1), tf.float32),\n",
    "                  \"label\": tf.FixedLenFeature((num_classes), tf.int64)}\n",
    "        parsed_features = tf.parse_single_example(record, features)\n",
    "        return parsed_features[\"query\"], parsed_features[\"passage\"],parsed_features[\"label\"]\n",
    "\n",
    "    def parser(fileName):\n",
    "        dataset = tf.data.TFRecordDataset(filenames = fileName, compression_type=\"ZLIB\")\n",
    "        parsed_features = dataset.map(parseEachRecord)\n",
    "        return parsed_features\n",
    "    \n",
    "    def getDatasetIterator(folderPath,batch_size,SHUFFLE_BATCH_SIZE,mode):\n",
    "        files = [os.path.join(folderPath, f) for f in os.listdir(folderPath)]\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(files).shuffle(buffer_size = len(files))\n",
    "        #dataset = tf.data.TFRecordDataset(filenames = fileName, compression_type=\"ZLIB\").shuffle(buffer_size = len(files))\n",
    "        #------Follow this order: map -> prefetch -> batch\n",
    "        dataset = dataset.flat_map(parser)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        dataset = dataset.prefetch(1)\n",
    "        iterator = dataset.make_initializable_iterator()\n",
    "        return iterator\n",
    "    \n",
    "   \n",
    "    iterator = getDatasetIterator(embeddingsFile,BATCH_SIZE,SHUFFLE_BATCH_SIZE,mode)\n",
    "    \n",
    "    queryfeatures,passagefeatures,y = iterator.get_next()\n",
    "    \n",
    "    model_spec =     {\n",
    "        'queryfeatures': queryfeatures,\n",
    "        'passagefeatures': passagefeatures,\n",
    "        'iterator_init_op': iterator.initializer,\n",
    "        \"y\":y\n",
    "    }\n",
    "    \n",
    "    with tf.variable_scope('model'):\n",
    "        y_conv = cnn_network(queryfeatures,passagefeatures,reuse = not is_training)\n",
    "        #tf.summary.scalar('batchSizeTmp', queryfeatures.shape)\n",
    "    \n",
    "    with tf.variable_scope('lossPerBatch'):\n",
    "        tmpMultiplier = params[\"WeightMultiplierPosClass\"]\n",
    "        weights = tf.multiply(tmpMultiplier, tf.cast(tf.argmax(y,-1),tf.float32)) + 1\n",
    "        cross_entropy = tf.reduce_mean(tf.losses.softmax_cross_entropy(onehot_labels=y,logits=y_conv,weights = weights))\n",
    "        #cross_entropy = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(targets=tf.cast(y,tf.float32),logits=y_conv,pos_weight = 9.0))\n",
    "        tf.summary.scalar('lossPerBatch', cross_entropy)\n",
    "\n",
    "    if is_training:\n",
    "        with tf.name_scope('AdamOptim'):\n",
    "            global_step = tf.train.get_or_create_global_step()\n",
    "            train_step = tf.train.AdamOptimizer(1e-4).minimize(loss = cross_entropy,global_step=global_step)\n",
    "\n",
    "    correct_pred = tf.equal(tf.argmax(y_conv,1),tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "    tf.summary.scalar('accuracyPerBatch', accuracy)\n",
    "    \n",
    "    \n",
    "    # -----------------------------------------------------------\n",
    "    # METRICS AND SUMMARIES\n",
    "    # Metrics for evaluation using tf.metrics (average over whole dataset)\n",
    "    with tf.variable_scope(\"metrics\"):\n",
    "        metrics = {\n",
    "            'accuracy': tf.metrics.accuracy(labels=tf.argmax(y,-1), predictions=tf.argmax(y_conv,-1)),\n",
    "            'loss': tf.metrics.mean(cross_entropy),\n",
    "            'auc':tf.metrics.auc(labels=tf.argmax(y,-1), predictions=tf.argmax(y_conv,-1))\n",
    "        }\n",
    "    \n",
    "    accuracyOverall,_ = metrics[\"accuracy\"]\n",
    "    lossOverall,_ = metrics[\"loss\"]\n",
    "    aucOVerall,_ = metrics[\"auc\"]\n",
    "    \n",
    "    tf.summary.scalar('accuracy', accuracyOverall)\n",
    "    tf.summary.scalar('loss',lossOverall )\n",
    "    tf.summary.scalar('auc', aucOVerall)\n",
    "    \n",
    "    # Group the update ops for the tf.metrics\n",
    "    update_metrics_op = tf.group(*[op for _, op in metrics.values()])\n",
    "\n",
    "    # Get the op to reset the local variables used in tf.metrics\n",
    "    metric_variables = tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES, scope=\"metrics\")\n",
    "    metrics_init_op = tf.variables_initializer(metric_variables)\n",
    "    \n",
    "    \n",
    "    # -----------------------------------------------------------\n",
    "    # MODEL SPECIFICATION\n",
    "    # Create the model specification and return it\n",
    "    # It contains nodes or operations in the graph that will be used for training and evaluation\n",
    "    variable_init_op = tf.group(*[tf.global_variables_initializer()])\n",
    "    model_spec['variable_init_op'] = variable_init_op\n",
    "    model_spec[\"predictions\"] = y_conv\n",
    "    model_spec['loss'] = cross_entropy\n",
    "    model_spec['accuracy'] = accuracy\n",
    "    model_spec['metrics_init_op'] = metrics_init_op\n",
    "    model_spec['metrics'] = metrics\n",
    "    model_spec['update_metrics'] = update_metrics_op\n",
    "    model_spec['summary_op'] = tf.summary.merge_all()\n",
    "\n",
    "    if is_training:\n",
    "        model_spec['train_op'] = train_step\n",
    "    \n",
    "    return model_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sess(sess, model_spec, num_steps, writer, params):\n",
    "    \"\"\"Train the model on `num_steps` batches\n",
    "\n",
    "    Args:\n",
    "        sess: (tf.Session) current session\n",
    "        model_spec: (dict) contains the graph operations or nodes needed for training\n",
    "        num_steps: (int) train for this number of batches\n",
    "        writer: (tf.summary.FileWriter) writer for summaries\n",
    "        params: (Params) hyperparameters\n",
    "    \"\"\"\n",
    "    # Get relevant graph operations or nodes needed for training\n",
    "    loss = model_spec['loss']\n",
    "    train_op = model_spec['train_op']\n",
    "    update_metrics = model_spec['update_metrics']\n",
    "    metrics = model_spec['metrics']\n",
    "    summary_op = model_spec['summary_op']\n",
    "    y_conv = model_spec[\"predictions\"]\n",
    "    global_step = tf.train.get_global_step()\n",
    "\n",
    "    # Load the training dataset into the pipeline and initialize the metrics local variables\n",
    "    sess.run(model_spec['iterator_init_op'])\n",
    "    sess.run(model_spec['metrics_init_op'])\n",
    "\n",
    "    # Use tqdm for progress bar\n",
    "    t = trange(num_steps)\n",
    "    for i in t:\n",
    "        # Evaluate summaries for tensorboard only once in a while\n",
    "        if i % params[\"save_summary_steps\"] == 0:\n",
    "            # Perform a mini-batch update\n",
    "            _, _, loss_val, summ, global_step_val = sess.run([train_op, update_metrics, loss,summary_op, global_step])\n",
    "            # Write summaries for tensorboard\n",
    "            #print(\"Global Step: \",global_step_val)\n",
    "            writer.add_summary(summ, global_step_val)\n",
    "        else:\n",
    "            _, _, loss_val = sess.run([train_op, update_metrics, loss])\n",
    "        # Log the loss in the tqdm progress bar\n",
    "        t.set_postfix(loss='{:05.3f}'.format(loss_val))\n",
    "        #print(\"Predictions\",sess.run([y_conv[0:10,]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sess(sess, model_spec, num_steps, writer=None, params=None):\n",
    "    \"\"\"Train the model on `num_steps` batches.\n",
    "\n",
    "    Args:\n",
    "        sess: (tf.Session) current session\n",
    "        model_spec: (dict) contains the graph operations or nodes needed for training\n",
    "        num_steps: (int) train for this number of batches\n",
    "        writer: (tf.summary.FileWriter) writer for summaries. Is None if we don't log anything\n",
    "        params: (Params) hyperparameters\n",
    "    \"\"\"\n",
    "    update_metrics = model_spec['update_metrics']\n",
    "    eval_metrics = model_spec['metrics']\n",
    "    global_step = tf.train.get_global_step()\n",
    "\n",
    "    # Load the evaluation dataset into the pipeline and initialize the metrics init op\n",
    "    sess.run(model_spec['iterator_init_op'])\n",
    "    sess.run(model_spec['metrics_init_op'])\n",
    "\n",
    "    # compute metrics over the dataset\n",
    "    for _ in range(num_steps):\n",
    "        sess.run(update_metrics)\n",
    "\n",
    "    # Get the values of the metrics\n",
    "    metrics_values = {k: v[0] for k, v in eval_metrics.items()}\n",
    "    metrics_val = sess.run(metrics_values)\n",
    "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_val.items())\n",
    "    logging.info(\"- Eval metrics: \" + metrics_string)\n",
    "\n",
    "    # Add summaries manually to writer at global_step_val\n",
    "    if writer is not None:\n",
    "        global_step_val = sess.run(global_step)\n",
    "        for tag, val in metrics_val.items():\n",
    "            summ = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=val)])\n",
    "            writer.add_summary(summ, global_step_val)\n",
    "\n",
    "    return metrics_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_spec, model_dir, params, restore_from):\n",
    "    import pandas as pd\n",
    "    \"\"\"Evaluate the model\n",
    "\n",
    "    Args:\n",
    "        model_spec: (dict) contains the graph operations or nodes needed for evaluation\n",
    "        model_dir: (string) directory containing config, weights and log\n",
    "        params: (Params) contains hyperparameters of the model.\n",
    "                Must define: num_epochs, train_size, batch_size, eval_size, save_summary_steps\n",
    "        restore_from: (string) directory or file containing weights to restore the graph\n",
    "    \"\"\"\n",
    "    # Initialize tf.Saver\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize the lookup table\n",
    "        #sess.run(model_spec['variable_init_op'])\n",
    "        # Reload weights from the weights subdirectory\n",
    "        save_path = os.path.join(model_dir, restore_from)\n",
    "        if os.path.isdir(save_path):\n",
    "            save_path = tf.train.latest_checkpoint(save_path)\n",
    "        saver.restore(sess, save_path)\n",
    "        \n",
    "        sess.run(model_spec['iterator_init_op'])\n",
    "        totalBatches = (104170//params[\"TEST_BATCH_SIZE\"]) + 1\n",
    "        \n",
    "        for index in range(totalBatches):        \n",
    "            predictions,query_id,passage_id = sess.run([model_spec[\"predictions\"],model_spec[\"query_id\"],model_spec[\"passage_id\"]])\n",
    "            \n",
    "            tmp = pd.DataFrame({\"query_id\":query_id[:,0],\"passage_id\":passage_id[:,0],\"predictions\":predictions[:,1]})\n",
    "            df = pd.concat([df,tmp],axis = 0)\n",
    "            #print(query_id[0:100])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(train_model_spec, eval_model_spec, model_dir, params):\n",
    "    \"\"\"Train the model and evaluate every epoch.\n",
    "\n",
    "    Args:\n",
    "        train_model_spec: (dict) contains the graph operations or nodes needed for training\n",
    "        eval_model_spec: (dict) contains the graph operations or nodes needed for evaluation\n",
    "        model_dir: (string) directory containing config, weights and log\n",
    "        params: (Params) contains hyperparameters of the model.\n",
    "                Must define: num_epochs, train_size, batch_size, eval_size, save_summary_steps\n",
    "        restore_from: (string) directory or file containing weights to restore the graph\n",
    "    \"\"\"\n",
    "    \n",
    "    max_query_words = params[\"max_query_words\"]\n",
    "    max_passage_words = params[\"max_passage_words\"]\n",
    "    emb_dim = params[\"emb_dim\"]\n",
    "    BATCH_SIZE = params[\"BATCH_SIZE\"]\n",
    "    EPOCHS = params[\"EPOCHS\"]\n",
    "    TotalTrainingdata = params[\"TotalTrainingdata\"]\n",
    "    num_classes = params[\"num_classes\"]\n",
    "    \n",
    "    # Initialize tf.Saver instances to save weights during training\n",
    "    last_saver = tf.train.Saver() # will keep last 5 epochs\n",
    "    best_saver = tf.train.Saver(max_to_keep=1)  # only keep 1 best checkpoint (best on eval)\n",
    "    begin_at_epoch = 0\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # Initialize model variables\n",
    "        sess.run(train_model_spec['variable_init_op'])\n",
    "\n",
    "        # For tensorboard (takes care of writing summaries to files)\n",
    "        train_writer = tf.summary.FileWriter(os.path.join(model_dir, 'train_summaries'), sess.graph)\n",
    "        eval_writer = tf.summary.FileWriter(os.path.join(model_dir, 'eval_summaries'), sess.graph)\n",
    "\n",
    "        best_eval_acc = 0.0\n",
    "        for epoch in range(EPOCHS):\n",
    "            # Run one epoch\n",
    "            logging.info(\"Epoch {}/{}\".format(epoch + 1, EPOCHS))\n",
    "            \n",
    "            numSteps = TotalTrainingdata // BATCH_SIZE\n",
    "            train_sess(sess, train_model_spec, numSteps, train_writer, params)\n",
    "\n",
    "            # Save weights\n",
    "            last_save_path = os.path.join(model_dir, 'last_weights', 'after-epoch')\n",
    "            last_saver.save(sess, last_save_path, global_step=epoch + 1)\n",
    "\n",
    "            metrics = evaluate_sess(sess, eval_model_spec, 1, eval_writer)\n",
    "\n",
    "            # If best_eval, best_save_path\n",
    "            eval_acc = metrics['auc']\n",
    "            if eval_acc >= best_eval_acc:\n",
    "                # Store new best accuracy\n",
    "                best_eval_acc = eval_acc\n",
    "                # Save weights\n",
    "                best_save_path = os.path.join(model_dir, 'best_weights', 'after-epoch')\n",
    "                best_save_path = best_saver.save(sess, best_save_path, global_step=epoch + 1)\n",
    "                logging.info(\"- Found new best accuracy, saving in {}\".format(best_save_path))\n",
    "                # Save best eval metrics in a json file in the model directory\n",
    "                best_json_path = os.path.join(model_dir, \"metrics_eval_best_weights.json\")\n",
    "                save_dict_to_json(metrics, best_json_path)\n",
    "\n",
    "            # Save latest eval metrics in a json file in the model directory\n",
    "            last_json_path = os.path.join(model_dir, \"metrics_eval_last_weights.json\")\n",
    "            save_dict_to_json(metrics, last_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"model/query/convLayer2/maxPool:0\", shape=(?, 2, 10, 2), dtype=float32)\n",
      "Tensor(\"model_1/query/convLayer2/maxPool:0\", shape=(?, 2, 10, 2), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/47176 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node model/passage/convLayer1/conv2D (defined at <ipython-input-2-917930d397ef>:9)  = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](AdamOptim/gradients/model/passage/convLayer1/conv2D_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer, model/passage/convLayer1/shared/weight/read)]]\n\t [[{{node metrics/auc/assert_less_equal/Assert/AssertGuard/Assert/Switch_1/_53}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_289_metrics/auc/assert_less_equal/Assert/AssertGuard/Assert/Switch_1\", tensor_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'model/passage/convLayer1/conv2D', defined at:\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\asyncio\\base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-d15a54dd4501>\", line 4, in <module>\n    train_model_spec = model_fn(\"train\",\"./TrainData_50\",params)\n  File \"<ipython-input-12-d8a7456011a3>\", line 49, in model_fn\n    y_conv = cnn_network(queryfeatures,passagefeatures,reuse = not is_training)\n  File \"<ipython-input-2-917930d397ef>\", line 51, in cnn_network\n    convPassage1 = tf.nn.relu(conv2D(passagefeatures,weight,[1,1,1,1]) + bias)\n  File \"<ipython-input-2-917930d397ef>\", line 9, in conv2D\n    return tf.nn.conv2d(x,W,strides=strides,padding='VALID',name=\"conv2D\")\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 1044, in conv2d\n    data_format=data_format, dilations=dilations, name=name)\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nUnknownError (see above for traceback): Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node model/passage/convLayer1/conv2D (defined at <ipython-input-2-917930d397ef>:9)  = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](AdamOptim/gradients/model/passage/convLayer1/conv2D_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer, model/passage/convLayer1/shared/weight/read)]]\n\t [[{{node metrics/auc/assert_less_equal/Assert/AssertGuard/Assert/Switch_1/_53}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_289_metrics/auc/assert_less_equal/Assert/AssertGuard/Assert/Switch_1\", tensor_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node model/passage/convLayer1/conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](AdamOptim/gradients/model/passage/convLayer1/conv2D_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer, model/passage/convLayer1/shared/weight/read)]]\n\t [[{{node metrics/auc/assert_less_equal/Assert/AssertGuard/Assert/Switch_1/_53}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_289_metrics/auc/assert_less_equal/Assert/AssertGuard/Assert/Switch_1\", tensor_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-d15a54dd4501>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0meval_model_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"eval\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"./ValidationData_50\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Starting training for {} epoch(s)\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"EPOCHS\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_model_spec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_model_spec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"./ModelLogs\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-e4db2a154f9b>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(train_model_spec, eval_model_spec, model_dir, params)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mnumSteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTotalTrainingdata\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0mtrain_sess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_model_spec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumSteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_writer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[1;31m# Save weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-43b759fda040>\u001b[0m in \u001b[0;36mtrain_sess\u001b[1;34m(sess, model_spec, num_steps, writer, params)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"save_summary_steps\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[1;31m# Perform a mini-batch update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msumm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate_metrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msummary_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[1;31m# Write summaries for tensorboard\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[1;31m#print(\"Global Step: \",global_step_val)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1346\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1350\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node model/passage/convLayer1/conv2D (defined at <ipython-input-2-917930d397ef>:9)  = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](AdamOptim/gradients/model/passage/convLayer1/conv2D_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer, model/passage/convLayer1/shared/weight/read)]]\n\t [[{{node metrics/auc/assert_less_equal/Assert/AssertGuard/Assert/Switch_1/_53}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_289_metrics/auc/assert_less_equal/Assert/AssertGuard/Assert/Switch_1\", tensor_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'model/passage/convLayer1/conv2D', defined at:\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\asyncio\\base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-d15a54dd4501>\", line 4, in <module>\n    train_model_spec = model_fn(\"train\",\"./TrainData_50\",params)\n  File \"<ipython-input-12-d8a7456011a3>\", line 49, in model_fn\n    y_conv = cnn_network(queryfeatures,passagefeatures,reuse = not is_training)\n  File \"<ipython-input-2-917930d397ef>\", line 51, in cnn_network\n    convPassage1 = tf.nn.relu(conv2D(passagefeatures,weight,[1,1,1,1]) + bias)\n  File \"<ipython-input-2-917930d397ef>\", line 9, in conv2D\n    return tf.nn.conv2d(x,W,strides=strides,padding='VALID',name=\"conv2D\")\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 1044, in conv2d\n    data_format=data_format, dilations=dilations, name=name)\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nUnknownError (see above for traceback): Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node model/passage/convLayer1/conv2D (defined at <ipython-input-2-917930d397ef>:9)  = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](AdamOptim/gradients/model/passage/convLayer1/conv2D_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer, model/passage/convLayer1/shared/weight/read)]]\n\t [[{{node metrics/auc/assert_less_equal/Assert/AssertGuard/Assert/Switch_1/_53}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_289_metrics/auc/assert_less_equal/Assert/AssertGuard/Assert/Switch_1\", tensor_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "#-----------------Main function for training\n",
    "\n",
    "tf.reset_default_graph()\n",
    "train_model_spec = model_fn(\"train\",\"./TrainData_50\",params)\n",
    "eval_model_spec = model_fn(\"eval\",\"./ValidationData_50\",params)\n",
    "logging.info(\"Starting training for {} epoch(s)\".format(params[\"EPOCHS\"]))\n",
    "train_and_evaluate(train_model_spec, eval_model_spec, \"./ModelLogs\", params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------Main function for testing\n",
    "\n",
    "tf.reset_default_graph()\n",
    "eval_model_spec = modelTest_fn(\"test\",\"./evalUnlabelledEmbeddings.tfrecords\",params)\n",
    "logging.info(\"Starting training for {} epoch(s)\".format(params[\"EPOCHS\"]))\n",
    "df = evaluate(eval_model_spec, \"./ModelLogs\", params,\"best_weights\")\n",
    "\n",
    "import csv\n",
    "tmp = (df.groupby('query_id')['predictions']\n",
    "       .apply(lambda x: \"\\t\".join([format(val, \"0.2f\") for val in x]))\n",
    "       .reset_index())\n",
    "\n",
    "tmp.to_csv(\"./answer.tsv\",index=False,sep= \"\\t\",header=None,quoting=csv.QUOTE_NONE,quotechar=\"\",  escapechar=\"\\\\\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
