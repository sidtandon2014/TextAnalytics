{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-090810e644f4>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 1.,  4.,  9.],\n",
      "       [28., 40., 54.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32,shape = [2,3])\n",
    "y = tf.placeholder(tf.float32,shape = [2,3])\n",
    "\n",
    "mulOp = tf.multiply(x,y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    val = sess.run([mulOp],feed_dict = {x:[[1,2,3],[4,5,6]], y:[[1,2,3],[7,8,9]]})\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"IteratorGetNext_3:0\", shape=(?, 12, 50, 1), dtype=float32)\n",
      "[[1135787]\n",
      " [1135787]\n",
      " [1135787]\n",
      " [1135787]\n",
      " [1135787]\n",
      " [1135787]] [[0]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [5]]\n",
      "[[1135787]\n",
      " [1135787]\n",
      " [1135787]\n",
      " [1135787]\n",
      " [ 281922]\n",
      " [ 281922]] [[6]\n",
      " [7]\n",
      " [8]\n",
      " [9]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "batch_size = 6\n",
    "max_query_words = 12\n",
    "max_passage_words = 50\n",
    "emb_dim = 100\n",
    "        \n",
    "def testDSParser(example_proto):\n",
    "    features = {\"query\": tf.FixedLenFeature((max_query_words,emb_dim,1), tf.float32),\n",
    "              \"passage\": tf.FixedLenFeature((max_passage_words,emb_dim,1), tf.float32),\n",
    "              \"query_id\": tf.FixedLenFeature((1), tf.int64),\n",
    "              \"passage_id\": tf.FixedLenFeature((1), tf.int64)}\n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    return parsed_features[\"query\"], parsed_features[\"passage\"],parsed_features[\"query_id\"],parsed_features[\"passage_id\"]\n",
    "\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(filenames = \"./trainEmbeddings.tfrecords\", compression_type=\"ZLIB\").shuffle(buffer_size = 1000)\n",
    "dataset = dataset.map(testDSParser)\n",
    "dataset = dataset.batch(batch_size)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "query,passage,query_id,passage_id = iterator.get_next()\n",
    "print(query)\n",
    "with tf.Session() as sess:\n",
    "    for i in range(2):\n",
    "        q,p,qid,pid = sess.run([query,passage,query_id,passage_id])\n",
    "        print(qid,pid)\n",
    "    #tmp = q[0].decode(\"utf-8\").split()\n",
    "    #print(np.array(tmp).reshape(12,50))\n",
    "    #print(len(q[0].decode(\"utf-8\").split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(fileName):\n",
    "        record = tf.data.TFRecordDataset(filenames = fileName, compression_type=\"ZLIB\")\n",
    "        features = {\"query\": tf.FixedLenFeature((max_query_words,emb_dim,1), tf.float32),\n",
    "                  \"passage\": tf.FixedLenFeature((max_passage_words,emb_dim,1), tf.float32),\n",
    "                  \"label\": tf.FixedLenFeature((num_classes), tf.int64)}\n",
    "        parsed_features = tf.parse_single_example(record, features)\n",
    "        return parsed_features[\"query\"], parsed_features[\"passage\"],parsed_features[\"label\"]\n",
    "    \n",
    "    def getDatasetIterator(folderPath,batch_size,SHUFFLE_BATCH_SIZE,mode):\n",
    "        files = [os.path.join(folderPath, f) for f in os.listdir(folderPath)]\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(files).shuffle(buffer_size = len(files))\n",
    "        #dataset = tf.data.TFRecordDataset(filenames = fileName, compression_type=\"ZLIB\").shuffle(buffer_size = len(files))\n",
    "        #------Follow this order: map -> prefetch -> batch\n",
    "        dataset = dataset.flat_map(parser)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        dataset = dataset.prefetch(1)\n",
    "        iterator = dataset.make_initializable_iterator()\n",
    "        return iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Stratified BAtch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[1, 0],\n",
      "       [1, 0]], dtype=int64)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "batch_size = 2\n",
    "max_query_words = 12\n",
    "max_passage_words = 50\n",
    "emb_dim = 50\n",
    "num_classes = 2\n",
    "        \n",
    "def parseEachRecord(record):\n",
    "    features = {\"query\": tf.FixedLenFeature((max_query_words,emb_dim,1), tf.float32),\n",
    "              \"passage\": tf.FixedLenFeature((max_passage_words,emb_dim,1), tf.float32),\n",
    "              \"label\": tf.FixedLenFeature((num_classes), tf.int64)}\n",
    "    parsed_features = tf.parse_single_example(record, features)\n",
    "    return parsed_features[\"query\"], parsed_features[\"passage\"],parsed_features[\"label\"]\n",
    "    \n",
    "def parserTextFile(fileName):\n",
    "    dataset = tf.data.TextLineDataset(fileName).map(lambda line: tf.decode_csv(line,record_defaults = [[1.]]))\n",
    "    return dataset\n",
    "\n",
    "def parser(fileName):\n",
    "    dataset = tf.data.TFRecordDataset(filenames = fileName, compression_type=\"ZLIB\")\n",
    "    parsed_features = dataset.map(parseEachRecord)\n",
    "    return parsed_features\n",
    "\n",
    "def getDatasetIterator(folderPath,batch_size):\n",
    "    files = [os.path.join(folderPath, f) for f in os.listdir(folderPath)]\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(files).shuffle(buffer_size = len(files))\n",
    "    #------Follow this order: map -> prefetch -> batch\n",
    "    dataset = dataset.flat_map(parser)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(1)\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    return iterator\n",
    "\n",
    "iterator = getDatasetIterator(\"./TrainData_50\",2)\n",
    "q,p,l = iterator.get_next()\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax([[0,1],[1,0]],1),tf.argmax(l,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run([iterator.initializer])\n",
    "    for i in range(1):\n",
    "        tmp = sess.run([l])\n",
    "        print(tmp)\n",
    "        #print(index)\n",
    "        \n",
    "    #tmp = sess.run([ds])\n",
    "    #print(type(tmp[0]))\n",
    "    #q,p,l = sess.run([query,passage,label])\n",
    "    #print(q.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'IteratorGetNext:0' shape=(?,) dtype=string>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "GloveEmbeddings = {}\n",
    "max_query_words = 12\n",
    "max_passage_words = 50\n",
    "emb_dim = 50\n",
    "\n",
    "def loadEmbeddings(embeddingfile):\n",
    "    global GloveEmbeddings,emb_dim\n",
    "\n",
    "    fe = open(embeddingfile,\"r\",encoding=\"utf-8\",errors=\"ignore\")\n",
    "    for line in fe:\n",
    "        tokens= line.strip().split()\n",
    "        word = tokens[0]\n",
    "        vec = tokens[1:]\n",
    "        vec = \" \".join(vec)\n",
    "        GloveEmbeddings[word]=vec\n",
    "    #Add Zerovec, this will be useful to pad zeros, it is better to experiment with padding any non-zero constant values also.\n",
    "    GloveEmbeddings[\"zerovec\"] = \"0.0 \"*emb_dim\n",
    "    fe.close()\n",
    "    \n",
    "loadEmbeddings(\"./glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['weather', 'hanoi', 'march', 'zerovec', 'zerovec', 'zerovec', 'zerovec', 'zerovec', 'zerovec', 'zerovec', 'zerovec', 'zerovec']\n",
      "608\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "f = open(\"./trainData.tsv\",\"r\",encoding=\"utf-8\",errors=\"ignore\")  # Format of the file : query_id \\t query \\t passage \\t label \\t passage_id\n",
    "    \n",
    "opts = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.ZLIB)\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "for line in f:\n",
    "    tokens = line.strip().lower().split(\"\\t\")\n",
    "    query_id,query,passage,label = int(tokens[0]),tokens[1],tokens[2],int(tokens[3])\n",
    "\n",
    "    wordsWithoutPunctuation = re.split('\\W+', query)\n",
    "    words = [word for word in wordsWithoutPunctuation if word not in stop_words and len(word) > 0]\n",
    "    if len(words) == 0:\n",
    "        words = wordsWithoutPunctuation\n",
    "\n",
    "    word_count = len(words)\n",
    "    remaining = max_query_words - word_count  \n",
    "    if(remaining>0):\n",
    "        words += [\"zerovec\"]*remaining # Pad zero vecs if the word count is less than max_query_words\n",
    "    words = words[:max_query_words] # trim extra words\n",
    "    print(words)\n",
    "    #create Query Feature vector \n",
    "    query_feature_vector = \"\"\n",
    "    for word in words:\n",
    "        if(word in GloveEmbeddings):\n",
    "            query_feature_vector += GloveEmbeddings[word]+\" \"\n",
    "        else:\n",
    "            query_feature_vector += GloveEmbeddings[\"zerovec\"]+\" \"  #Add zerovec for OOV terms\n",
    "    query_feature_vector = query_feature_vector.strip() \n",
    "    print(len(query_feature_vector.split(\" \")))\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15131936021751349076\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3215186329\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 6959646577381805404\n",
      "physical_device_desc: \"device: 0, name: Quadro M1000M, pci bus id: 0000:01:00.0, compute capability: 5.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"./eval1_unlabelled.tsv\",sep = \"\\t\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104170, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "524188 4717692\n"
     ]
    }
   ],
   "source": [
    "print(len(data[data[3] == 1]),len(data[data[3] == 0]))\n",
    "#-------Total count: z\n",
    "#-------1 count: 524188, 0 Count: 4717692"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "weight = tf.get_variable(name = \"weight\",shape = (2,2), initializer=tf.initializers.truncated_normal(stddev= 0.1))\n",
    "\n",
    "y = [[1,0],[0,1]]\n",
    "\n",
    "loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(onehot_labels=y,logits=weight))\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss = loss,global_step=global_step,name = \"tmp\")\n",
    "\n",
    "global_Step = tf.train.get_global_step()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    _,val = sess.run([train_step,global_Step])\n",
    "    print(sess.run([global_Step])   )\n",
    "    \n",
    "    for index in range(5):\n",
    "        step,val = sess.run([train_step,global_Step])\n",
    "        #val = sess.run([global_Step])    \n",
    "        print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([2, 0, 1], dtype=int64)]\n",
      "[array([[1., 2., 3.],\n",
      "       [3., 2., 1.],\n",
      "       [7., 8., 1.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "y = [[1,2,3],[3,2,1],[7,8,1]]\n",
    "\n",
    "x = tf.argmax(y,-1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run([x]))\n",
    "    print(sess.run([tf.cast(y,tf.float32)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf auc: [0.74999976, 0.74999976]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "y_true = np.array([0, 0, 1, 1])\n",
    "y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "\n",
    "import tensorflow as tf\n",
    "tf\n",
    "auc, update_op = tf.metrics.auc(y_true, y_scores)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    print(\"tf auc: {}\".format(sess.run([auc, update_op])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStratifiedDatasetIterator(fileName,batch_size,SHUFFLE_BATCH_SIZE,mode):\n",
    "    dataset = tf.data.TFRecordDataset(filenames = fileName, compression_type=\"ZLIB\").shuffle(buffer_size = SHUFFLE_BATCH_SIZE)\n",
    "    #------Follow this order: map -> prefetch -> batch\n",
    "    dataset = dataset.map(parser)\n",
    "    ds_pos = dataset.filter(lambda a,b,c: tf.reshape(tf.equal(tf.argmax(c,-1), 1), []))\n",
    "    ds_neg = dataset.filter(lambda a,b,c: tf.reshape(tf.equal(tf.argmax(c,-1), 0), []))\n",
    "\n",
    "    dataset = tf.data.Dataset.zip((ds_pos, ds_neg))\n",
    "\n",
    "    # Each input element will be converted into a two-element `Dataset` using\n",
    "    # `Dataset.from_tensors()` and `Dataset.concatenate()`, then `Dataset.flat_map()`\n",
    "    # will flatten the resulting `Dataset`s into a single `Dataset`.\n",
    "    dataset = dataset.flat_map(\n",
    "        lambda ex_pos, ex_neg: tf.data.Dataset.from_tensors(ex_pos).concatenate(\n",
    "            tf.data.Dataset.from_tensors(ex_neg)))\n",
    "\n",
    "    dataset = dataset.batch(batch_size=batch_size)\n",
    "    dataset = dataset.prefetch(1)\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    return iterator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "dataset = tf.dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
